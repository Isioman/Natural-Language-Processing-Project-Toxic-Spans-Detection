{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Toxic Spans Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isioman/Natural-Language-Processing-Project-Toxic-Spans-Detection/blob/main/Toxic_Spans_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpZYUfT96GqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f0afa3-bfff-4c66-9af6-1439cf898808"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install profanity-filter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: profanity-filter in /usr/local/lib/python3.7/dist-packages (1.3.3)\n",
            "Requirement already satisfied: ordered-set-stubs<0.2.0,>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from profanity-filter) (0.1.3)\n",
            "Requirement already satisfied: more-itertools<9.0,>=8.0 in /usr/local/lib/python3.7/dist-packages (from profanity-filter) (8.11.0)\n",
            "Requirement already satisfied: ordered-set<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from profanity-filter) (3.1.1)\n",
            "Requirement already satisfied: spacy<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from profanity-filter) (2.2.4)\n",
            "Requirement already satisfied: ruamel.yaml<0.16.0,>=0.15.89 in /usr/local/lib/python3.7/dist-packages (from profanity-filter) (0.15.100)\n",
            "Requirement already satisfied: poetry-version<0.2.0,>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from profanity-filter) (0.1.5)\n",
            "Requirement already satisfied: cached-property<2.0,>=1.5 in /usr/local/lib/python3.7/dist-packages (from profanity-filter) (1.5.2)\n",
            "Requirement already satisfied: redis<4.0,>=3.2 in /usr/local/lib/python3.7/dist-packages (from profanity-filter) (3.5.3)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from profanity-filter) (1.8.2)\n",
            "Requirement already satisfied: tomlkit<0.6.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from poetry-version<0.2.0,>=0.1.3->profanity-filter) (0.5.11)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic<2.0,>=1.3->profanity-filter) (3.10.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->profanity-filter) (1.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->profanity-filter) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->profanity-filter) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->profanity-filter) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->profanity-filter) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->profanity-filter) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->profanity-filter) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->profanity-filter) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->profanity-filter) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->profanity-filter) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->profanity-filter) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->profanity-filter) (4.62.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0,>=2.0->profanity-filter) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0,>=2.0->profanity-filter) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0,>=2.0->profanity-filter) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0,>=2.0->profanity-filter) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0,>=2.0->profanity-filter) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0,>=2.0->profanity-filter) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0,>=2.0->profanity-filter) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STgqfMaRHizh"
      },
      "source": [
        "import io\n",
        "import os\n",
        "import nltk \n",
        "import spacy\n",
        "import string\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch as torch\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from io import FileIO\n",
        "from ast import literal_eval\n",
        "from google.colab import auth\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "from googleapiclient.discovery import build\n",
        "from profanity_filter import ProfanityFilter\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from transformers import  BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,precision_recall_fscore_support\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwXQlB2-Bexm"
      },
      "source": [
        "# Set seed values for all python libraries\n",
        "def set_seed(seed_value):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4vLNvMBBini"
      },
      "source": [
        "# Set seed value of 42 for all python libraries\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56KWKx0WJFB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc5d6568-2e19-48eb-b656-2cca3f715770"
      },
      "source": [
        "# Mount the google drive for reading and writing input output files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6tR0BKRDWRJ"
      },
      "source": [
        "# Use GPU if available else use regular CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iQ6-jMCsLcA",
        "outputId": "49c2ef08-2e69-47d0-b5aa-8335d4ac821f"
      },
      "source": [
        "# Credit: Bridget McInnes code of BERT_Examples.ipynb \n",
        "#  Get the files from the google drive\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Get train data file\n",
        "file_id = '16yiT1Wl4CPxt8-3DTYnXzZV1s2AcFxcT'  # Train Data file on the Google Drive\n",
        "downloaded = io.FileIO(\"tsd_train.csv\", 'w')\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "downloader = MediaIoBaseDownload(downloaded, request)\n",
        "done = False\n",
        "while done is False:\n",
        "  status, done = downloader.next_chunk()\n",
        "  print(\"Download {}%.\".format(int(status.progress() * 100)))\n",
        "\n",
        "# Get train data file\n",
        "file_id = '1vyQHb3TJJ9daawHU5GBI49T9PfH12bQb'  # Trial Data file on the Google Drive\n",
        "downloaded = io.FileIO(\"tsd_trail.csv\", 'w')\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "downloader = MediaIoBaseDownload(downloaded, request)\n",
        "done = False\n",
        "while done is False:\n",
        "  status, done = downloader.next_chunk()\n",
        "  print(\"Download {}%.\".format(int(status.progress() * 100)))\n",
        "\n",
        "# Get test data file\n",
        "file_id = '1w_CDQjo9Qd4URP293cpS-et6k4VdLLwW'  # Test Data file on the Google Drive\n",
        "downloaded = io.FileIO(\"tsd_test.csv\", 'w')\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "downloader = MediaIoBaseDownload(downloaded, request)\n",
        "done = False\n",
        "while done is False:\n",
        "  status, done = downloader.next_chunk()\n",
        "  print(\"Download {}%.\".format(int(status.progress() * 100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download 100%.\n",
            "Download 100%.\n",
            "Download 100%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeMjU9PIjzwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "724894a0-d076-4b90-ddad-0f6965823947"
      },
      "source": [
        "# Download stopwords and create a ProfanityFilter using Spacy\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "profanity_filter = ProfanityFilter(nlps={'en': nlp})\n",
        "nlp.add_pipe(profanity_filter.spacy_component, last=True)\n",
        "stopwords = nlp.Defaults.stop_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLD6LXHRU1DR"
      },
      "source": [
        "#Credit: https://github.com/ipavlopoulos/toxic_spans/blob/master/evaluation/fix_spans.py\n",
        "#This method is provided the task organizers to extract contiguous ranges in the given span \n",
        "# E.g. [1, 2, 3, 5, 6, 7] -> [(1,3), (5,7)]\n",
        "def contiguous_ranges(span_list):\n",
        "    output = []\n",
        "    for _, span in itertools.groupby(\n",
        "        enumerate(span_list), lambda p: p[1] - p[0]):\n",
        "        span = list(span)\n",
        "        output.append((span[0][1], span[-1][1]))\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdH1X9N5UxyF"
      },
      "source": [
        "#This method will perform minor edits by trimming the spans and removing singletons\n",
        "#Credit: https://github.com/ipavlopoulos/toxic_spans/blob/master/evaluation/fix_spans.py\n",
        "SPECIAL_CHARACTERS = string.whitespace\n",
        "def fix_spans(spans, text, special_characters=SPECIAL_CHARACTERS):\n",
        "  # print(spans)\n",
        "  # print(text)\n",
        "  cleaned = []\n",
        "  for begin, end in contiguous_ranges(spans):\n",
        "      while text[begin] in special_characters and begin < end:\n",
        "          begin += 1\n",
        "      while text[end] in special_characters and begin < end:\n",
        "          end -= 1\n",
        "      if end - begin > 1:\n",
        "          cleaned.extend(range(begin, end + 1))\n",
        "  return cleaned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NJgk71gVi2p"
      },
      "source": [
        "#This method is used to invoke the fix_spans method to perform minor edits to the spans\n",
        "def clean_spans(spans, posts):\n",
        "  clean_span_list = list()\n",
        "  for index, span in enumerate(spans):\n",
        "    clean_span_list.append(fix_spans(span, posts[index]))\n",
        "  return clean_span_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpGg--6PYfMO"
      },
      "source": [
        "# This method is used to compute the number of spans in the train, trial and test data\n",
        "def number_of_spans(spans):\n",
        "  empty_spans_count = 0\n",
        "  single_spans_count = 0\n",
        "  multi_spans_count = 0\n",
        "\n",
        "  for index, span in enumerate(spans):\n",
        "    if len(span) == 0:\n",
        "      empty_spans_count += 1\n",
        "    else:\n",
        "      list_of_spans = contiguous_ranges(span)\n",
        "      single_spans_count += len(list_of_spans) == 1\n",
        "      multi_spans_count += len(list_of_spans) > 1\n",
        "\n",
        "  return empty_spans_count, single_spans_count, multi_spans_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv6QGNaGH5zZ"
      },
      "source": [
        "# Method to load the training data and compute few exploratory metrics\n",
        "def load_train_dataset():\n",
        "  toxic_data = pd.read_csv('tsd_train.csv')\n",
        "  toxic_data[\"spans\"] = toxic_data.spans.apply(literal_eval)\n",
        "  texts, spans = toxic_data[\"text\"], toxic_data[\"spans\"]\n",
        "\n",
        "  #Put the text and spans in list\n",
        "  toxic_text_list = texts.values.tolist()\n",
        "  toxic_spans_list = spans.values.tolist()\n",
        "\n",
        "  #Clean the spans to remove singletons and trimming spaces. Code provided by SemEval organizers\n",
        "  cleaned_spans = clean_spans(toxic_spans_list, toxic_text_list)\n",
        "\n",
        "  # Get number of spans\n",
        "  empty_spans_count, single_spans_count, multi_spans_count = number_of_spans(toxic_spans_list)\n",
        "\n",
        "  print('Total Training Samples:', len(toxic_text_list))\n",
        "  print('Empty Spans:', empty_spans_count)\n",
        "  print('Single Spans:', single_spans_count)\n",
        "  print('Multi Spans:', multi_spans_count)\n",
        "  print('*************************************************************')\n",
        "\n",
        "  return toxic_text_list, cleaned_spans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNlE-J9Jh6-J"
      },
      "source": [
        "# Method to load the trial data and compute few exploratory metrics\n",
        "def load_trial_dataset():\n",
        "  toxic_data = pd.read_csv('tsd_trail.csv')\n",
        "  toxic_data[\"spans\"] = toxic_data.spans.apply(literal_eval)\n",
        "  texts, spans = toxic_data[\"text\"], toxic_data[\"spans\"]\n",
        "\n",
        "  #Put the text and spans in list\n",
        "  toxic_text_list = texts.values.tolist()\n",
        "  toxic_spans_list = spans.values.tolist()\n",
        "\n",
        "  #Clean the spans to remove singletons and trimming spaces. Code provided by SemEval organizers\n",
        "  cleaned_spans = clean_spans(toxic_spans_list, toxic_text_list)\n",
        "\n",
        "  # Get number of spans\n",
        "  empty_spans_count, single_spans_count, multi_spans_count = number_of_spans(toxic_spans_list)\n",
        "\n",
        "  print('Total Validation Samples:', len(toxic_text_list))\n",
        "  print('Empty Spans:', empty_spans_count)\n",
        "  print('Single Spans:', single_spans_count)\n",
        "  print('Multi Spans:', multi_spans_count)\n",
        "  print('*************************************************************')\n",
        "\n",
        "  return toxic_text_list, cleaned_spans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNg5Y7q_M9R3"
      },
      "source": [
        "# Method to load the test data and compute few exploratory metrics\n",
        "def load_test_dataset():\n",
        "  toxic_data = pd.read_csv('tsd_test.csv')\n",
        "  toxic_data[\"spans\"] = toxic_data.spans.apply(literal_eval)\n",
        "  texts, spans = toxic_data[\"text\"], toxic_data[\"spans\"]\n",
        "\n",
        "  #Put the text and spans in list\n",
        "  toxic_text_list = texts.values.tolist()\n",
        "  toxic_spans_list = spans.values.tolist()\n",
        "\n",
        "  #Clean the spans to remove singletons and trimming spaces. Code provided by SemEval organizers\n",
        "  cleaned_spans = clean_spans(toxic_spans_list, toxic_text_list)\n",
        "\n",
        "  # Get number of spans\n",
        "  empty_spans_count, single_spans_count, multi_spans_count = number_of_spans(toxic_spans_list)\n",
        "\n",
        "  print('Total Test Samples:', len(toxic_text_list))\n",
        "  print('Empty Spans:', empty_spans_count)\n",
        "  print('Single Spans:', single_spans_count)\n",
        "  print('Multi Spans:', multi_spans_count)\n",
        "  print('*************************************************************')\n",
        "\n",
        "  return toxic_text_list, cleaned_spans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FYJ3V0ufRzA"
      },
      "source": [
        "# This method will compute the maximum length of a post in train, trial and test data\n",
        "def max_post_length(toxic_posts):\n",
        "  max_length = 0\n",
        "  idx_max_len_post = 0\n",
        "  for index, post in enumerate(toxic_posts):\n",
        "    length = len(post)\n",
        "    if length > max_length:\n",
        "      max_length = length\n",
        "      idx_max_len_post = index\n",
        "  return idx_max_len_post, max_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlsuUKuiM3G1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f329f044-7913-4a34-a9e3-cf27702101ce"
      },
      "source": [
        "# Invoke method to load train, trial and test data\n",
        "train_posts, train_spans = load_train_dataset()\n",
        "trail_posts, trail_spans = load_trial_dataset()\n",
        "test_posts, test_spans = load_test_dataset()\n",
        "\n",
        "print('Train data max post length:', max_post_length(train_posts))\n",
        "print('Trial data max post length:',max_post_length(trail_posts))\n",
        "print('Test data max post length:',max_post_length(test_posts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Training Samples: 7939\n",
            "Empty Spans: 485\n",
            "Single Spans: 5370\n",
            "Multi Spans: 2084\n",
            "*************************************************************\n",
            "Total Validation Samples: 690\n",
            "Empty Spans: 43\n",
            "Single Spans: 448\n",
            "Multi Spans: 199\n",
            "*************************************************************\n",
            "Total Test Samples: 2000\n",
            "Empty Spans: 394\n",
            "Single Spans: 1407\n",
            "Multi Spans: 199\n",
            "*************************************************************\n",
            "Train data max post length: (212, 1000)\n",
            "Trial data max post length: (324, 998)\n",
            "Test data max post length: (713, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3CIeWlHu8z7"
      },
      "source": [
        "# This is used to collect all the toxic words in the train, trial and test data\n",
        "def get_toxic_words_count(posts):\n",
        "  toxic_dict = dict()\n",
        "  for post in posts:\n",
        "    sentences = sent_tokenize(post)\n",
        "    for sentence in sentences:\n",
        "      word_tokens = word_tokenize(sentence)\n",
        "      for word in word_tokens:\n",
        "        doc = nlp(word)\n",
        "        is_toxic_word = doc._.is_profane\n",
        "        if is_toxic_word:\n",
        "          if word not in toxic_dict.keys():\n",
        "            toxic_dict[word] = 1\n",
        "          else:\n",
        "            count = toxic_dict.get(word)\n",
        "            toxic_dict[word] = count + 1   \n",
        "  return toxic_dict\n",
        "  print(toxic_dict)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jASOCmt0f07p"
      },
      "source": [
        "# Generate a word cloud for the most frequent toxic words\n",
        "def toxic_word_cloud(toxic_word_dict):\n",
        "  toxic_keys = toxic_words_dict.keys()\n",
        "\n",
        "  wordcloud_stopwords = set(STOPWORDS)\n",
        "  text = ' '.join([str(elem) for elem in toxic_keys])\n",
        "  wordcloud = WordCloud(width = 1200, height = 800,stopwords = stopwords,).generate(text)\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout(pad = 0)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl7e1b7cgs00"
      },
      "source": [
        "# #Get toxic words for training data and generate word cloud\n",
        "# train_toxic_words_dict = get_toxic_words_count(train_posts)\n",
        "# toxic_word_cloud(train_toxic_words_dict)\n",
        "\n",
        "# #Get toxic words for validation data and generate word cloud\n",
        "# trail_toxic_words_dict = get_toxic_words_count(trail_posts)\n",
        "# toxic_word_cloud(trail_toxic_words_dict)\n",
        "\n",
        "# #Get toxic words for test data and generate word cloud\n",
        "# test_toxic_words_dict = get_toxic_words_count(test_posts)\n",
        "# toxic_word_cloud(test_toxic_words_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufj813n33ODS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19e1ce6-5c44-4c12-f82c-d97a59967128"
      },
      "source": [
        "# Define BERT Tokenizer and BERT Model\n",
        "bertTokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
        "bertModel = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
        "bertModel.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKSGmZ8FCKY0"
      },
      "source": [
        "#Tokenize Training, Validation and Testing data\n",
        "train_posts_encodings = bertTokenizer(train_posts, return_offsets_mapping=True,  padding=True, truncation=True, return_tensors=\"pt\")\n",
        "trial_posts_encodings = bertTokenizer(trail_posts, return_offsets_mapping=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "test_posts_encodings = bertTokenizer(test_posts, return_offsets_mapping=True, padding=True, truncation=True, return_tensors=\"pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPpgolKUiIug"
      },
      "source": [
        "# This method is used match the spans after BERT tokenizes words into sub-tokens\n",
        "def encode_spans(encoding, span):\n",
        "  labels = [0] * len(encoding.tokens)\n",
        "\n",
        "  toxic_indices = set(span)\n",
        "  for i, offset in enumerate(encoding.offsets):\n",
        "    if offset == (0, 0):\n",
        "      labels[i] = -100    \n",
        "    else:\n",
        "      for k in range(offset[0], offset[1]):\n",
        "        if k in toxic_indices: \n",
        "          labels[i] = 1\n",
        "          break\n",
        "  \n",
        "  return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFOmBxr2NgUI"
      },
      "source": [
        "# Update train, trial and test spans to match with the sub-tokens\n",
        "train_spans_updated = [encode_spans(train_posts_encodings[i], span) for i, span in enumerate(train_spans)]\n",
        "trial_spans_updated = [encode_spans(trial_posts_encodings[i], span) for i, span in enumerate(trail_spans)]\n",
        "test_spans_updated = [encode_spans(test_posts_encodings[i], span) for i, span in enumerate(test_spans)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra99zmLZNtie"
      },
      "source": [
        "#Define the class for Toxic Spans Dataset\n",
        "class ToxicDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iCdqSbzOLu4"
      },
      "source": [
        "# Crete ToxicDataset object for training, trial and test datasets\n",
        "train_dataset = ToxicDataset(train_posts_encodings, train_spans_updated)\n",
        "trial_dataset = ToxicDataset(trial_posts_encodings, trial_spans_updated)\n",
        "test_dataset = ToxicDataset(test_posts_encodings, test_spans_updated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaYtpWpJOVXu"
      },
      "source": [
        "# Offset Mappings are not passed to the BERT Model. So Remove them from the encodings\n",
        "train_offset_mapping = train_posts_encodings.pop(\"offset_mapping\") \n",
        "trial_offset_mapping = trial_posts_encodings.pop(\"offset_mapping\")\n",
        "test_offset_mapping = test_posts_encodings.pop(\"offset_mapping\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN1vbc6xaxNN"
      },
      "source": [
        "# This method will do the post-processing steps as defined in the modle\n",
        "def post_process_predictions(predicted_offsets, posts_offset_mappings, posts_encodings):\n",
        "    # Compute softmax output for all the predictions\n",
        "    predictions = predicted_offsets.predictions.argmax(-1)\n",
        "    pred_offsets_scores = predicted_offsets.predictions\n",
        "  \n",
        "    final_processed_char_offsets = list()\n",
        "\n",
        "    # Iterate over each offset mapping, and its corresponding prediction\n",
        "    for index, (offset, prediction) in enumerate(zip(posts_offset_mappings, predictions)):\n",
        "        post_processed_offset = list()\n",
        "        # Get tokens for each post\n",
        "        offset_tokens = posts_encodings[index].tokens\n",
        "          \n",
        "        current_token_index = 1\n",
        "        # Iterate over all the token to find the post processed character offsets\n",
        "        while current_token_index < len(offset_tokens):\n",
        "            # print('***********************************************************')\n",
        "            # print('Current Token: {} being looked at index: {} :'.format(offset_tokens[current_token_index], current_token_index)) \n",
        "           \n",
        "            # If the token being processed is '[SEP]' then break the loop\n",
        "            if offset_tokens[current_token_index] == '[SEP]':\n",
        "                # print('SEP token found')\n",
        "                break\n",
        "            char_offsets = list()\n",
        "    \n",
        "            # Get current and next tokens\n",
        "            current_token = offset_tokens[current_token_index]\n",
        "      \n",
        "            # This if condition will handle the last token in the tokens list\n",
        "            if current_token_index + 1 == len(offset_tokens):\n",
        "                # print('Last token being monitored')\n",
        "                current_token_prediction = prediction[current_token_index]\n",
        "                previous_token_prediction = prediction[current_token_index - 1]\n",
        "                if current_token_prediction == 1:  \n",
        "                    if previous_token_prediction == 1:\n",
        "                        prev_token_offset_values = get_last_offset_value(offset, current_token_index - 1)\n",
        "                        char_offsets.extend(prev_token_offset_values)\n",
        "    \n",
        "                current_token_offset_values = get_offset_values_from_range(offset, current_token_index)\n",
        "                break\n",
        "            else:   # This else will handle all the tokens except the last one\n",
        "                # print('Start current token index:', current_token_index)\n",
        "                # print('Current Token: {} being looked at index: {} :'.format(current_token, current_token_index))     \n",
        "                next_token = offset_tokens[current_token_index + 1]        \n",
        "    \n",
        "                # Get current and next token predictions\n",
        "                previous_token_prediction = prediction[current_token_index - 1]\n",
        "                current_token_prediction = prediction[current_token_index]\n",
        "                next_token_prediction = prediction[current_token_index + 1]\n",
        "    \n",
        "                # check if next token starts with '##'\n",
        "                is_current_token_starts_with_hash = current_token.startswith('##')\n",
        "                is_next_token_starts_with_hash = next_token.startswith('##')\n",
        "        \n",
        "                # print('start token with hash:', is_current_token_starts_with_hash)\n",
        "                # print('next token with hash:', is_next_token_starts_with_hash)\n",
        "    \n",
        "                if (not is_current_token_starts_with_hash) and (not is_next_token_starts_with_hash):\n",
        "                    if current_token_prediction == 1 and next_token_prediction == 1:\n",
        "                        # If Previous token prediction = 1 then get index value of the last element in the range\n",
        "                        if previous_token_prediction == 1:\n",
        "                            prev_token_offset_values = get_last_offset_value(offset, current_token_index - 1)\n",
        "                            char_offsets.extend(prev_token_offset_values)\n",
        "                            # print('Previous Token offset values:', prev_token_offset_values)\n",
        "                          \n",
        "                        current_token_offset_values = get_offset_values_from_range(offset, current_token_index)\n",
        "                        next_token_offset_values = get_offset_values_from_range(offset, current_token_index + 1)\n",
        "                        end_token_offset_values = get_last_offset_value(offset, current_token_index)\n",
        "                        # print('End Token offset value:', end_token_offset_values)\n",
        "    \n",
        "                        char_offsets.extend(current_token_offset_values) \n",
        "                        char_offsets.extend(end_token_offset_values)\n",
        "                        char_offsets.extend(next_token_offset_values)\n",
        "                        # print('Consecute tokens offset values{[]},{[]},{[]}:', current_token_offset_values, end_token_offset_values, next_token_offset_values)\n",
        "    \n",
        "                        current_token_index = current_token_index + 2\n",
        "    \n",
        "                    elif current_token_prediction == 1 and next_token_prediction != 1:\n",
        "                        if previous_token_prediction == 1:\n",
        "                            prev_token_offset_values = get_last_offset_value(offset, current_token_index - 1)\n",
        "                            char_offsets.extend(prev_token_offset_values)\n",
        "    \n",
        "                        current_token_offset_values = get_offset_values_from_range(offset, current_token_index)\n",
        "                        char_offsets.extend(current_token_offset_values)\n",
        "                        current_token_index = current_token_index + 1\n",
        "                    else:\n",
        "                        current_token_index = current_token_index + 1\n",
        "    \n",
        "                elif is_current_token_starts_with_hash or is_next_token_starts_with_hash:  \n",
        "                    hash_char_offsets = list()  \n",
        "                    hash_char_predictions = list()   \n",
        "                    # print('Token with hash found:', next_token) \n",
        "    \n",
        "                    if previous_token_prediction == 1:\n",
        "                        prev_token_offset_values = get_last_offset_value(offset, current_token_index - 1)\n",
        "                        hash_char_offsets.extend(prev_token_offset_values)\n",
        "    \n",
        "                    current_token_offset_value = get_offset_values_from_range(offset, current_token_index)\n",
        "                    current_token_prediction = prediction[current_token_index]\n",
        "                    hash_char_offsets.extend(current_token_offset_value)\n",
        "                    hash_char_predictions.append(current_token_prediction)\n",
        "    \n",
        "                    # print('Hash Char offsets:',hash_char_offsets)\n",
        "    \n",
        "                    current_token_index = current_token_index + 1\n",
        "                    while current_token_index < len(offset_tokens):\n",
        "                        current_token = offset_tokens[current_token_index]\n",
        "                        # print('In while loop token:', current_token)\n",
        "                        if(current_token.startswith('##')):\n",
        "                            current_token_offset_value = get_offset_values_from_range(offset, current_token_index)\n",
        "                            current_token_prediction = prediction[current_token_index]\n",
        "                            hash_char_offsets.extend(current_token_offset_value)\n",
        "                            hash_char_predictions.append(current_token_prediction)\n",
        "                            # print('Hash Char offsets:',hash_char_offsets)\n",
        "                            current_token_index = current_token_index + 1\n",
        "                        else:\n",
        "                            break\n",
        "                        # print('Predictions with ##:', hash_char_predictions)\n",
        "                        if 1 in hash_char_predictions:\n",
        "                            char_offsets.extend(hash_char_offsets)                  \n",
        "    \n",
        "            post_processed_offset.extend(char_offsets)  \n",
        "        final_processed_char_offsets.append(post_processed_offset)  \n",
        "        # print('Post Processed Offsets:', final_processed_char_offsets)\n",
        "    return final_processed_char_offsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pRAzoFMPxFG"
      },
      "source": [
        "# This method will compute the offset value in a given range\n",
        "def get_offset_values_from_range(offset_mapping, index):\n",
        "  offset_values = []\n",
        "  first_offset = offset_mapping[index][0]\n",
        "  second_offset = offset_mapping[index][1]\n",
        "\n",
        "  # print('Offset being looked at:{},{}'.format(first_offset, second_offset))\n",
        "  for i in range(first_offset, second_offset):\n",
        "    offset_values.append(i)\n",
        "  return offset_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66ezsJ4ZIfJE"
      },
      "source": [
        "# This method will return the last offset value for a given offset mapping\n",
        "def get_last_offset_value(offset_mapping, index):\n",
        "  offset_values = []\n",
        "  first_offset = offset_mapping[index][1]\n",
        "  second_offset = offset_mapping[index + 1][0]\n",
        "\n",
        "  # print('Offset being looked at:{},{}'.format(first_offset, second_offset))\n",
        "  for i in range(first_offset, second_offset):\n",
        "    offset_values.append(i)\n",
        "  return offset_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHUZut-hKk2D"
      },
      "source": [
        "# This method will compute per post precision\n",
        "def compute_per_post_precision(pred_spans, true_spans):\n",
        "  pred_spans_length = len(pred_spans)\n",
        "  true_spans_length = len(true_spans)\n",
        "  \n",
        "  # Return Precision = 1 if both length of predicted and true spans are of length 0 \n",
        "  # else return Precision = 0\n",
        "  if true_spans_length == 0:\n",
        "    if pred_spans_length == 0:\n",
        "      return 1.0\n",
        "    else:\n",
        "     return 0.0\n",
        "  \n",
        "  # Return zero precision when predicted spans length is zero\n",
        "  if pred_spans_length == 0:\n",
        "    return 0.0\n",
        "\n",
        "  spans_intersection = set(pred_spans).intersection(set(true_spans))\n",
        "  precision = len(spans_intersection) / pred_spans_length\n",
        "\n",
        "  return float(precision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HJdKx6nKpTD"
      },
      "source": [
        "# This method will compute per post recall value\n",
        "def compute_per_post_recall(pred_spans, true_spans):\n",
        "  pred_spans_length = len(pred_spans)\n",
        "  true_spans_length = len(true_spans)\n",
        "  \n",
        "  # Return Recall = 1 if both length of predicted and true spans are of length 0 \n",
        "  # else return Recall = 0\n",
        "  if true_spans_length == 0:\n",
        "    if pred_spans_length == 0:\n",
        "      return 1.0\n",
        "    else:\n",
        "     return 0.0\n",
        "  \n",
        "  # Return zero precision when predicted spans length is zero\n",
        "  if pred_spans_length == 0:\n",
        "    return 0.0\n",
        "\n",
        "  spans_intersection = set(pred_spans).intersection(set(true_spans))\n",
        "  recall = len(spans_intersection) / true_spans_length\n",
        "\n",
        "  return float(recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iokqgvXrKr4A"
      },
      "source": [
        "# This method will compute per post f1-score\n",
        "def compute_per_post_f1_score(pred_spans, true_spans):\n",
        "  precision = compute_per_post_precision(pred_spans, true_spans)\n",
        "  recall = compute_per_post_recall(pred_spans, true_spans)\n",
        "\n",
        "  pred_spans_length = len(pred_spans)\n",
        "  true_spans_length = len(true_spans)\n",
        "\n",
        "  if(true_spans_length == 0):\n",
        "    return 1.0 if pred_spans_length == 0 else 0.0\n",
        "  \n",
        "  if pred_spans_length == 0:\n",
        "    return 0.0\n",
        "\n",
        "  pred_spans_set = set(pred_spans)\n",
        "  true_spans_set = set(true_spans)\n",
        "\n",
        "  spans_intersection = pred_spans_set.intersection(true_spans_set)\n",
        "  spans_intersection_length = len(spans_intersection)\n",
        "\n",
        "  f1_score = (2 * spans_intersection_length) / (pred_spans_length + true_spans_length)\n",
        "\n",
        "  return float(f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEZ_-ujIbG-Q"
      },
      "source": [
        "# This method will compute system level precision, recall and f1-score values\n",
        "def compute_all_evaluation_metrics(pred_spans, true_spans):\n",
        "  total_precision = [compute_per_post_precision(pred_span, true_span) for pred_span, true_span in zip(pred_spans, true_spans)]\n",
        "  total_recall = [compute_per_post_recall(pred_span, true_span) for pred_span, true_span in zip(pred_spans, true_spans)]\n",
        "  total_f1_score = [compute_per_post_f1_score(pred_span, true_span) for pred_span, true_span in zip(pred_spans, true_spans)]\n",
        "\n",
        "  mean_precision = np.mean(total_precision)\n",
        "  mean_recall = np.mean(total_recall)\n",
        "  mean_f1_score = np.mean(total_f1_score)\n",
        "  \n",
        "  return mean_precision, mean_recall, mean_f1_score  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yVRtnXZbKcc"
      },
      "source": [
        "# This method will compute the evaluation metrics for the predicted and true spans\n",
        "def compute_metrics(pred_offsets, true_spans, offset_mappings, posts_encodings):  \n",
        "  pred_spans = post_process_predictions(pred_offsets, offset_mappings, posts_encodings)\n",
        "  precision, recall, f1_score = compute_all_evaluation_metrics(pred_spans, true_spans)\n",
        "\n",
        "  return precision, recall, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_T3RZQsZBku",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa9147fb-c7a5-4a86-e954-337059229a91"
      },
      "source": [
        "plt.figure(dpi=600)\n",
        "plt.rc('axes', labelsize=16)\n",
        "plt.rc('font', size=13)   \n",
        "#This method will plot the confusion matrix\n",
        "def plot_confusion_matrices(encodings, predictions, labels):\n",
        "\n",
        "  y_true, y_pred = [], []\n",
        "  \n",
        "  for i, (pred, gold) in enumerate(zip(predictions, labels)):\n",
        "    sep_token = 1\n",
        "    tokens = encodings[i].tokens\n",
        "    #print('Tokens:', tokens)\n",
        "    #print('Predictions:', pred)\n",
        "    #print('True Spans:', gold)\n",
        "    while tokens[sep_token] != '[SEP]':\n",
        "      sep_token += 1\n",
        "    y_true.extend(gold[1: sep_token])\n",
        "    y_pred.extend(pred[1: sep_token])\n",
        "\n",
        "  true_length = len(y_true);\n",
        "  y_pred = y_pred[:true_length]\n",
        "  # Normal confusion matrix\n",
        "  cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "  labels = ['Neutral', 'Toxic']\n",
        "  \n",
        "  ax = plt.axes()\n",
        "  sns_plot = sns.heatmap(cf_matrix / np.sum(cf_matrix), \n",
        "                        annot=True, fmt='.2%',\n",
        "                        xticklabels=labels, yticklabels=labels, \n",
        "                        ax = ax,\n",
        "                        cmap=\"YlGnBu\")\n",
        "\n",
        "  ax.set_xlabel('Predicted')\n",
        "  ax.set_ylabel('Actual')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 3600x2400 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZOkvHaSO8y5"
      },
      "source": [
        "# Configure Training Arguments for training the BERT Model\n",
        "training_args = TrainingArguments(\n",
        "  output_dir = '/drive/MyDrive/',\n",
        "  num_train_epochs = 2,                 # total number of training epochs\n",
        "  per_device_train_batch_size = 16,     # batch size per device during training\n",
        "  per_device_eval_batch_size = 16,      # batch size for evaluation\n",
        "  warmup_steps = 500,                   # number of warmup steps for learning rate scheduler\n",
        "  weight_decay = 0.01,                  # strength of weight decay\n",
        "  do_eval = True,                       # whether to run evaluation on the val set\n",
        "  evaluation_strategy = \"steps\",        # evaluation is done (and logged) every logging_steps \n",
        "  learning_rate = 2e-5,                 # 5e-5 is default learning rate\n",
        "  disable_tqdm = False,                 # remove tqdm statements to reduce clutter\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHQ03FJGPtlp"
      },
      "source": [
        "# Configure the Training object for training the BERT Model\n",
        "trainer = Trainer(\n",
        "  model=bertModel,                  # configure the model that needs to be trained\n",
        "  args=training_args,               # configure the training arguments\n",
        "  train_dataset=train_dataset,      # Initialize with the training dataset object \n",
        "  eval_dataset=trial_dataset,       # Initialize with the trial dataset object\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYR9dH8aJEib"
      },
      "source": [
        " #import torch\n",
        "#torch.cuda.empty_cache()\n",
        "#torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdcqagTpP4bI"
      },
      "source": [
        "# Train the BERT Model\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2dZZLi9P6yO"
      },
      "source": [
        "# Evaluate the BERT Model on trial and test data\n",
        "trial_predictions = trainer.predict(trial_dataset)\n",
        "test_predictions = trainer.predict(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8wmdSkHoDj3"
      },
      "source": [
        "# Post-process the predictions\n",
        "trial_processed_preds = post_process_predictions(trial_predictions, trial_offset_mapping, trial_posts_encodings)\n",
        "test_processed_preds = post_process_predictions(test_predictions, test_offset_mapping, test_posts_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8972dQr_0T4"
      },
      "source": [
        "# Compute final metrics\n",
        "trial_metrics = compute_metrics(trial_predictions, trail_spans, trial_offset_mapping, trial_posts_encodings)\n",
        "test_metrics = compute_metrics(test_predictions, test_spans, test_offset_mapping, test_posts_encodings)\n",
        "\n",
        "print(\"******Validation Evaluation Metrics**********\")\n",
        "print('Precision:{:.3f}'.format(trial_metrics[0]))\n",
        "print('Recall:{:.3f}'.format(trial_metrics[1]))\n",
        "print('F1-Score:{:.3f}'.format(trial_metrics[2]))\n",
        "\n",
        "print(\"*********Test Evaluation Metrics*************\")\n",
        "print('Precision:{:.3f}'.format(test_metrics[0]))\n",
        "print('Recall:{:.3f}'.format(test_metrics[1]))\n",
        "print('F1-Score:{:.3f}'.format(test_metrics[2]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5STEF4qlwD6"
      },
      "source": [
        "#plot_confusion_matrices(test_posts_encodings, test_processed_preds, test_spans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T1SgzDBC4wC"
      },
      "source": [
        "# Save Evaluation Metrics to File for trial data\n",
        "with open('/drive/MyDrive/evaluation_metrics_trial_data.txt', \"w\") as writer:\n",
        "  precision = trial_metrics[0]\n",
        "  recall = trial_metrics[1]\n",
        "  f1_score = trial_metrics[2]\n",
        "\n",
        "  writer.write(f\"Precision:{str(precision)} \\n\")\n",
        "  writer.write(f\"Recall: {str(recall)} \\n\")\n",
        "  writer.write(f\"F1_score: {str(f1_score)} \\n\")\n",
        "writer.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6j1A9zNEY71"
      },
      "source": [
        "# Save Evaluation Metrics to File for test data\n",
        "with open('/drive/MyDrive/evaluation_metrics_test_data.txt', \"w\") as writer:\n",
        "  precision = test_metrics[0]\n",
        "  recall = test_metrics[1]\n",
        "  f1_score = test_metrics[2]\n",
        "\n",
        "  writer.write(f\"Precision:{str(precision)} \\n\")\n",
        "  writer.write(f\"Recall: {str(recall)} \\n\")\n",
        "  writer.write(f\"F1_score: {str(f1_score)} \\n\")\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SCxi0GNPkdp"
      },
      "source": [
        "# Save Predicted Spans for Trial data in a text file\n",
        "with open('/drive/MyDrive/trial_predicted_spans.txt', \"w\") as writer:\n",
        "  for pred in trial_processed_preds:\n",
        "    pred_str = ' '.join([str(elem) for elem in pred])\n",
        "    pred_str = \"[\" + pred_str + \"]\"\n",
        "    writer.write(f\"{str(pred_str)} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT4p6s8QPlQa"
      },
      "source": [
        "# Save Predicted Spans for Test data in a text file\n",
        "with open('/drive/MyDrive/test_predicted_spans.txt', \"w\") as writer:\n",
        "  for pred in test_processed_preds:\n",
        "    pred_str = ' '.join([str(elem) for elem in pred])\n",
        "    pred_str = \"[\" + pred_str + \"]\"\n",
        "    writer.write(f\"{str(pred_str)} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKusdBUrCTe2"
      },
      "source": [
        "# Save Text of predicted spans for trial data\n",
        "trial_pred_output_file = '/drive/MyDrive/trial_predicted_text.txt'\n",
        "with open(trial_pred_output_file, \"w\") as fileWriter:\n",
        "  for toxic_post, predicted_span in zip(trail_posts, trial_processed_preds):\n",
        "    pred_span_ranges = contiguous_ranges(predicted_span)\n",
        "    toxic_span_text = list()\n",
        "    for span_range in pred_span_ranges:\n",
        "      toxic_span_text.append(toxic_post[span_range[0]:span_range[1] + 1])\n",
        "    fileWriter.write(f\"Original Toxic Post:{toxic_post} \\n\")\n",
        "    fileWriter.write(f\"Predicted Toxic Span Offset:{predicted_span} \\n\")\n",
        "    fileWriter.write(f\"Predicted Toxic Text:{str(toxic_span_text)} \\n\")\n",
        "    fileWriter.write(f\"*************************************************\\n\")\n",
        "  fileWriter.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvyapzp1ILAP"
      },
      "source": [
        "# Save Text of predicted spans for test data\n",
        "test_pred_output_file = '/drive/MyDrive/test_predicted_text.txt'\n",
        "with open(test_pred_output_file, \"w\") as fileWriter:\n",
        "  for toxic_post, predicted_span in zip(test_posts, test_processed_preds):\n",
        "    pred_span_ranges = contiguous_ranges(predicted_span)\n",
        "    toxic_span_text = list()\n",
        "    for span_range in pred_span_ranges:\n",
        "      toxic_span_text.append(toxic_post[span_range[0]:span_range[1] + 1])\n",
        "    fileWriter.write(f\"Original Toxic Post:{toxic_post} \\n\")\n",
        "    fileWriter.write(f\"Predicted Toxic Span Offset:{predicted_span} \\n\")\n",
        "    fileWriter.write(f\"Predicted Toxic Text:{str(toxic_span_text)} \\n\")\n",
        "    fileWriter.write(f\"*************************************************\\n\")\n",
        "  fileWriter.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBYtghXH1pmg"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}