## Toxic Spans Detection
Rapid growth and increased interaction on social media promoted good awareness and as well caused some damage to society. One major concern is the increase in negative comments and hate speeches posted on social media and online discussion forums. A traditional approach is to identify these harsh comments manually by human moderators and then either filter or remove the comments. This will help promote a safe environment for other users to participate in online discussions. However, the rapid growth in volumes of the data generated on social media and other platforms makes it tedious and complicated for human moderators to identify toxic posts. There is a pressing need to automate the process of annotating such offensive posts. Majority of the currently available toxicity detection datasets and models focus on classifying entire comment or document as toxic. They do not identify the actual sequence of words or toxic spans that actually make the text toxic. This necessity has motivated the SemEval-2021 organizers to design a task called as “Toxic Spans Detection”.

#### Below are the steps to run the implementation

- **Step 1:** The full implementation for the system can be found on Google colab. Click on the icon [![Colab](https://colab.research.google.com/assets/colab-badge.svg, target = "_blank")](https://colab.research.google.com/github/Isioman/Natural-Language-Processing-Project-Toxic-Spans-Detection/blob/main/Toxic_Spans_Detection_Project.ipynb) to redirect you to the program.

 - **Step 2:** Select **"Run all"** option to execute the code
 <img src="https://github.com/Isioman/Natural-Language-Processing-Project-Toxic-Spans-Detection/blob/main/Images/Run all option.jpg" width="600">
 
 - **Step 3**: To view the saved files, you can find it in the google drive linked to the Colab.
 
 #### Side Notes:
 1. A common error that can occur is excedding the allocation of GPU. To solve this:
   - Rerun the model by selecting the options in Runtime -> Restart and Run all.
 <br/>
  <img src="https://github.com/Isioman/Natural-Language-Processing-Project-Toxic-Spans-Detection/blob/main/Images/5.png" width="400" height="400">
  
 2. Other procedures to look out for include:
    - Ensuring the drive is always mounted for easy reading and writing of data.
    
 3. Ensure the cell corresponding Mount to Drive is executed, we need to provide a verification code.
    - Click on the hyperlink that is shown.
   <br/>
   <img src="https://github.com/Isioman/Natural-Language-Processing-Project-Toxic-Spans-Detection/blob/main/Images/Mount Google Drive - 1.jpg" width="600">
   
### Contributions and Questions
#### Contact: Contributors
- Madhu Kumar Dogiparthy | Graduate Student at VCU, Virginia, USA | dogiparthym@vcu.edu
- Goodness Isioma Nwabueze | Graduate Student at VCU, Virginia, USA | nwabuezeg@vcu.edu

We are open to your contributions and questions. 
